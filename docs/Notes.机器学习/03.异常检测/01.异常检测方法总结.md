---
title: 异常检测方法
date: 2022-03-01 14:18:24
permalink: /pages/1ac963/
categories:
  - 异常检测
  - 机器学习
tags:
  - 异常检测
  - 机器学习
---


[[TOC]]

# 异常检测方法

**同比，是指在相邻时段中的某一相同时间点进行比较。**

**环比，是指在同一时段中的相邻时间点进行比较。**

## 1 短期环比（SS）

对于时间序列（是指将同一统计指标的数值按其发生的时间先后顺序排列而成的数列）来说，$T$时刻的数值对于$T-1$时刻有很强的依赖性。比如流量在8:00很多，在8:01时刻的概率是很大的，但是如果07:01时刻对于8:01时刻影响不是很大。

首先，我们可以使用最近时间窗口（$T$）内的数据遵循某种趋势的现象来做文章。比如我们将$T$设置为7，则我们取检测值（$now\_value$）和过去7个（记为$i$）点进行比较，如果大于阈值我们将$count$加1，如果$count$超过我们设置的$count\_num$，则认为该点是异常点。
$$
count(\sum_{i=0}^T(now\_value-i)>threshold)>count\_num
$$


上面的公式涉及到$threshold$和$count\_num$两个参数，$count\_num$可以根据的需求进行设置，比如对异常敏感，可以设置$count\_num$小一些，而如果对异常不敏感，可以将$count\_num$设置的大一些。

### 1.1 动态阈值 

通常阈值设置方法会参考过去一段时间内的均值、最大值以及最小值，我们也同样应用此方法。取过去一段时间（比如$T$窗口）的平均值、最大值以及最小值，然后取$max-avg$和$avg−min$的最小值。之所以取最小值的原因是让筛选条件设置的宽松一些，让更多的值通过此条件，减少一些漏报的事件。
$$
threshold=min(max-avg,avg-min)
$$

### 1.2 窗口特征

#### 1.2.1统计特征

##### 3-sigma

一个很直接的异常判定思路是，拿最新3个datapoint的平均值（tail_avg方法）和整个序列比较，看是否偏离总体平均水平太多。怎样算“太多”呢，因为standard deviation表示集合中元素到mean的平均偏移距离，因此最简单就是和它进行比较。在normal distribution（正态分布）中，99.73%的数据都在偏离mean 3个σ (standard deviation 标准差) 的范围内。如果某些datapoint到mean的距离超过这个范围，则认为是异常的。

##### z score

标准分，一个个体到集合mean的偏离，以标准差为单位，表达个体距mean相对“平均偏离水平（std dev表达）”的偏离程度，常用来比对来自不同集合的数据。

在模型中，z_score用来衡量窗口数据中，中间值的偏离程度。

> **算法流程：**
>
> 1. 排除最后一个值
> 2. 求剩余序列的平均值
> 3. 全序列减去上面这个平均值
> 4. 求剩余序列的标准差
> 5. （ 中间三个数的平均值-全序列均值）/ 全序列标准差

##### Grubbs格拉斯测试                          

Grubbs测试是一种从样本中找出outlier的方法，所谓outlier，是指样本中偏离平均值过远的数据，他们有可能是极端情况下的正常数据，也有可能是测量过程中的错误数据。使用Grubbs测试需要总体是正态分布的。

> **算法流程：**
>
> 1. 样本从小到大排序
> 2. 求样本的mean和std.dev
> 3. 计算min/max与mean的差距，更大的那个为可疑值
> 4. 求可疑值的z-score (standard score)，如果大于Grubbs临界值，那么就是outlier；

Grubbs临界值可以查表得到，它由两个值决定：检出水平$\alpha$（越严格越小），样本数量$n$。排除outlier，对剩余序列循环做 1-4 步骤。由于这里需要的是异常判定，只需要判断tail_avg是否outlier即可。

##### moving average（移动平均）

给定一个时间序列和窗口长度$N$，moving average等于当前data point之前$N$个点（包括当前点）的平均值。不停地移动这个窗口，就得到移动平均曲线。

##### cumulative moving average（累加移动平均）

设$\{x_i:i≥1\}$是观察到的数据序列。 累积移动平均线是所有数据的未加权平均值。 如果若干天的值是$x_1,…,x_i$，那么

$$
CMA_i = \frac{x_1+...+x_i}{i}
$$
当有新的值$x_i+1$，那么累积移动平均为

$$
\begin{aligned}
CMA_{i+1}&=\frac{x_1+...+x_i+x_{i+1}}{i+1}\\
&=\frac{x_{i+1}+n\cdot CMA_i}{i+1}\\
&=CMA_i+ \frac{x_{i+1}-CMA_i}{i+1}
\end{aligned}
$$


##### weighted moving average（加权移动平均）

加权移动平均值是先前$w$个数据的加权平均值。 假设$\sum_{j=0}^{w-1}weight_j=1$，其中$weight_j>0$，则加权移动平均值为
$$
WMA_i=\sum_{j=0}^{w-1}weight_j\cdot x_{i-j}
$$
一般

$$
weight_j=\frac{w-j}{w+(w-1)+...+1},for\quad 0\leq j\leq w-1
$$


所以，
$$
WMA_i=\frac{wx_i+(w-1)x_{i-1}+...+2x_{i-w+2}+x_{i-w+1}}{w+(w+1)+...+1}
$$


##### exponential weighted moving average（指数加权移动平均）

指数移动与移动平均有些不同：

1. 并没有时间窗口，用的是从时间序列第一个data point到当前data point之间的所有点。
2. 每个data point的权重不同，离当前时间点越近的点的权重越大，历史时间点的权重随着离当前时间点的距离呈指数衰减，从当前data point往前的data point，权重依次为$\alpha,\alpha(1-\alpha),\alpha(1-\alpha)^2,...,\alpha(1-\alpha)^n$

该算法可以检测一个异常较短时间后发生另外一个异常的情况，异常持续一段时间后可能被判定为正常。

PS：https://www.jianshu.com/p/a2dbd47b3f1a

##### double exponential smoothing（双指数平滑）

假设${Y_t:t≥1}$是观测数据序列，有两个与双指数平滑相关的方程：

$$
S_t=αY_t+(1−α)(S_{t−1}+b_{t−1})\\
b_t=β(S_t−S_{t−1})+(1−β)b_{t−1}
$$
其中$\alpha \in[0,1]$,$\alpha$是数据平滑因子，$\beta\in[0,1]$,$\beta$是趋势平滑因子。

这里，初始值为$S_1=Y_1$，$b_1$有三种可能：

$$
b_1=Y_2-Y_1\\
b_1=\frac{(Y_2-Y_1)+(Y_3-Y_2)+(Y_4-Y_3)}{3}=\frac{Y_4-Y_1}{3}\\
b_1=\frac{Y_n-Y_1}{n-1}\\
$$
预测：

- 单个时刻：$F_{t+1}=S_t+b_t$
- m个时刻：$F_{t+m}=S_t+mb_t$

##### triple exponential smoothing（三指数平滑）（Holt-Winters）

- 乘法模型  multiplicative seasonality

  设${Y_t:t≥1}$是观察到的数据序列，则三指数平滑为

$$
S_t=α\frac{Y_t}{c_{t−L}}+(1−α)(S_{t−1}+b_{t−1}),overall smoothing\\
b_t=β(S_t−S_{t−1})+(1−β)b_{t−1},trend smoothing\\
c_t=γ\frac{Y_t}{S_t}+(1−γ)c_{t−L},seasonalsmoothing\\
$$

  其中$α∈[0,1]$是数据平滑因子，$β∈[0,1]$是趋势平滑因子， $γ∈[0,1]$是季节变化平滑因子。

  预测$m$个时刻：$F_{t+m}=(S_t+mb_t)c_{(t−L+m) mod L}$

  初始值设定：

- 加法模型  additive seasonality

  设${Yt:t≥1}$是观察到的数据序列，则三指数平滑为

  $$
  S_t = \alpha(Y_t-c_{t-L})+(1-\alpha)(S_{t-1}+b_{t-1}),overall\quad smoothing\\
  b_t = \beta(S_t-S_{t-1})+(1-\beta)b_{t-1},trend smoothing\\
  c_t = \gamma(Y_t-S_{t-1}-b_{t-1})+(1-\gamma)c_{t-L},seasonal smoothing
  $$
  
  
  其中$α∈[0,1]$是数据平滑因子，$β∈[0,1]$是趋势平滑因子， $γ∈[0,1]$是季节变化平滑因子。
  
  预测m个时刻：$F_{t+m}=S_t+mb_t+c_{(t−L+m) mod L}$

##### stddev from average（平均值-标准差）

该算法类似于3sigma准则。当数据服从高斯分布时，数值分布在$(μ−3σ,μ+3σ)$区间内的概率为99.74。所以，可以这么认为，当数据分布区间超过这个区间时，即可认为是异常数据。该算法使用$(t−mean)/std$ 作为特征，用于衡量中间三个值的平均值相对于3σ的距离。

该算法的特点是可以有效屏蔽“在一个点上突变到很大的异常值但在下一个点回落到正常水平”的情况，即屏蔽单点毛刺：因为它使用的是3个点的均值（有效缓和突变），和整个序列比较（均值可能被异常值拉大），导致判断正常。

**算法流程：**

1. 求窗口数据的平均值。 (mean)
2. 求窗口数据的标准差。 (std)
3. 求窗口数据中间3个值的平均值。（t）
4. 使用$(t−mean)/std$作为特征。

##### stddev from moving average（移动平均-标准差）

先求出最后一个点处的指数加权移动平均值，然后再用最新的点和三倍方差方法求异常。

##### stddev from ewma（指数加权移动平均-标准差）

类似于特征3，不过在计算$(t−mean)/std$时，使用的mean，std分别为对窗口数据进行移动加权平均后的平均值以及方差。

##### histogram bins

该算法和以上都不同，它首先将timeseries划分成15个宽度相等的直方，然后判断tail_avg所在直方内的元素是否<=20，如果是，则异常。

直方的个数和元素个数判定需要根据自己的metrics调整，不然在数据量小的时候很容易就异常了。

##### median absolute deviation（中位数绝对偏差）

**median：**大部分情况下我们用mean来表达一个集合的平均水平（average），但是在某些情况下存在少数极大或极小的outlier，拉高或拉低了（skew）整体的mean，造成估计的不准确。此时可以用median（中位数）代替mean描述平均水平。Median的求法很简单，集合排序中间位置即是，如果集合总数为偶数，则取中间二者的平均值。

**median of deviation（MAD）：**同mean一样，对于median我们也需要类似standard deviation这样的指标来表达数据的紧凑/分散程度，即偏离average的平均距离，这就是MAD。MAD顾名思义，是deviation的median，而此时的deviation = abs( 个体 – median )，避免了少量outlier对结果的影响，更robust。

绝对中位差实际求法是用原数据减去中位数后得到的新数据的绝对值的中位数。

1. 原数据-中位值=新数据
2. 新数据的绝对值的中位数作为特征

##### mean subtraction cumulation（平均值减法累积？）

该特征类似于3-sigma准则。

**算法流程**

1. 排除全序列（暂称为all）最后一个值（last datapoint），求剩余序列（暂称为rest，0..length-2）的mean；
2. rest序列中每个元素减去rest的mean，再求标准差；
3. 求窗口数据中间点到rest mean的距离，即 abs(last datapoint – rest mean)；

##### first hour average（前若干-平均值）

和上述算法基本一致，只是比较对象不是整个序列，而是开始一个小时（其实这种这种思想可以推广，只要是时间序列刚开始的一段时间即可）的以内的数据，求出这段时间的均值和标准差和尾部数据（新产生的数据）用三倍方差的方法比较即可。

#### 1.2.2 熵特征

为什么要研究时间序列的熵呢？请看下面两个时间序列：

时间序列（1）：(1,2,1,2,1,2,1,2,1,2,…)

时间序列（2）：(1,1,2,1,2,2,2,2,1,1,…)

在时间序列（1）中，1 和 2 是交替出现的，而在时间序列（2）中，1 和 2 是随机出现的。在这种情况下，时间序列（1）则更加确定，时间序列（2）则更加随机。并且在这种情况下，两个时间序列的统计特征，例如均值，方差，中位数等等则是几乎一致的，说明用之前的统计特征并不足以精准的区分这两种时间序列。

通常来说，要想描述一种确定性与不确定性，**熵（entropy）**是一种不错的指标。对于离散空间而言，一个系统的熵（entropy）可以这样来表示：

$$
entropy(X)=−K\sum_{i=1}^\infty P\{x=x_i\}ln(P\{x=x_i\}).
$$
$K$是和单位选取相关的任意常数。

如果一个系统的熵（entropy）越大，说明这个系统就越混乱；如果一个系统的熵越小，那么说明这个系统就更加确定。

提到时间序列的熵特征，一般来说有几个经典.的例子，那就是 **binned entropy**，**approximate entropy**，**sample entropy**。下面来一一介绍时间序列中这几个经典的熵。

##### Binned Entropy

从熵的定义出发，可以考虑把时间序列$X_T$的取值进行分桶的操作。例如，可以把$[min(X_T),max(X_T)]$这个区间等分为十个小区间，那么时间序列的取值就会分散在这十个桶中。根据这个等距分桶的情况，就可以计算出这个概率分布的熵（entropy）。i.e. Binned Entropy 就可以定义为：

$$
binned\quad entropy(X)=−\sum _{k=0}^{min(maxbin,len(X))}p_k ln(p_k)⋅1(p_k>0)
$$
其中 $p_k$ 表示时间序列 $X_T$ 的取值落在第 $k$ 个桶的比例（概率），$maxbin$ 表示桶的个数， $len(X_T)=T$  表示时间序列 $X_T$ 的长度。

如果一个时间序列的 Binned Entropy 较大，说明这一段时间序列的取值是较为均匀的分布在 $[min(X_T),max(X_T)]$之间的；如果一个时间序列的 Binned Entropy 较小，说明这一段时间序列的取值是集中在某一段上的。

##### Approximate Entropy

回到本节的问题，如何判断一个时间序列是否具备某种趋势还是随机出现呢？这就需要介绍 Approximate Entropy 的概念了，Approximate Entropy 的思想就是把一维空间的时间序列提升到高维空间中，通过高维空间的向量之间的距离或者相似度的判断，来推导出一维空间的时间序列是否存在某种趋势或者确定性。那么，我们现在可以假设时间序列 $X_N:{x_1,⋯,x_N}$的长度是$N$，同时 Approximate Entropy 函数拥有两个参数， $m$ 与 $r$ ，下面来详细介绍 Approximate Entropy 的算法细节。

1. 固定两个参数，正整数 $m$ 和正数 $r$，正整数 $m$ 是为了把时间序列进行一个片段的提取，正数 $r$ 是表示时间序列距离的某个参数。i.e. 需要构造新的 $m$ 维向量如下：
$$
  X_1(m)=(x_1,⋯,x_m)\in \mathbb{R}^m,\\
  X_i(m)=(x_i,⋯,x_{m+i−1})\in \mathbb{R}^m,\\
  X_{N−m+1}(m)=(x_{N−m+1},⋯,x_N)\in \mathbb{R}^m.
$$

2. 通过新的向量$X_1(m),⋯,X_{N−m+1(m)}$，可以计算出哪些向量与 $X_i$ 较为相似。即，
   $$
   C^m_i(r)=(number\quad of\quad X_j(m)\quad such\quad that\quad d(X_i(m),X_j(m))\leq r)/(N−m+1),
   $$
   在这里，距离$d$选择$L^1$，$L^2$，$L^p$，$L^{\infty}$范数。在这个场景下，距离$d$通常选择为$L^{\infty}$范数。

3. 考虑函数$\Phi ^m(r)=(N−m+1)^{−1}\cdot \sum _{i=1}^{N−m+1}ln(C^m_i(r))$

4. Approximate Entropy 可以定义为：
   $$
   ApEn(m,r)=\Phi ^m(r)−\Phi ^{m+1}(r)
   $$
   

   > **Remark:**
   >
   > 1. 正整数 mm 一般可以取值为 22 或者 33， r>0r>0 会基于具体的时间序列具体调整；
   > 2. 如果某条时间序列具有很多重复的片段（repetitive pattern）或者自相似性（self-similarity pattern），那么它的 Approximate Entropy 就会相对小；反之，如果某条时间序列几乎是随机出现的，那么它的 Approximate Entropy 就会相对较大。

##### Sample Entropy

除了 Approximate Entropy，还有另外一个熵的指标可以衡量时间序列，那就是 Sample Entropy，通过自然对数的计算来表示时间序列是否具备某种自相似性。

按照以上 Approximate Entropy 的定义，可以基于$m$与$r$定义两个指标$A$与$B$，分别是

$$
A=\#\{vector\quad pairs\quad having\quad d(X_i(m+1),X_j(m+1))<r\quad of\quad length\quad m+1 \}\\
B=\#\{vector\quad pairs\quad having\quad d(X_i(m),X_j(m))<r\quad of\quad length\quad m \}
$$
其中，$\#$ 表示集合的元素个数。根据度量 $d$ （无论是$L^1$，$L^2$，$L^\infty$ ）的定义可以知道 $A\leq B$，因此 Sample Entropy 总是非负数，即

$$
SampEn=−ln(A/B)\geq 0
$$
备注：

1. Sample Entropy 总是非负数；
2. Sample Entropy 越小表示该时间序列具有越强的自相似性（self similarity）。
3. 通常来说，在 Sample Entropy 的参数选择中，可以选择$m=2,r=0.2×std$。

#### 1.2.3 分段特征

即使时间序列有一定的自相似性（self-similarity），能否说明这两条时间序列就完全相似呢？其实答案是否定的，例如：两个长度都是 1000 的时间序列，

时间序列（1）：$ [1,2] * 500$

时间序列（2）：$[1,2,3,4,5,6,7,8,9,10] * 100$

其中，时间序列（1）是 1 和 2 循环的，时间序列（2）是 1~10 这样循环的，它们从图像上看完全是不一样的曲线，并且它们的 Approximate Entropy 和 Sample Entropy 都是非常小的。那么问题来了，有没有办法提炼出信息，从而表示它们的不同点呢？答案是肯定的。

首先，我们可以回顾一下 Riemann 积分和 Lebesgue 积分的定义和不同之处。按照下面两幅图所示，Riemann 积分是为了算曲线下面所围成的面积，因此把横轴划分成一个又一个的小区间，按照长方形累加的算法来计算面积。而 Lebesgue 积分的算法恰好相反，它是把纵轴切分成一个又一个的小区间，然后也是按照长方形累加的算法来计算面积。

![img](http://minio.vancode.top/vancode/algorithm/ad/clip1541314641.png)

之前的 Binned Entropy 方案是根据值域来进行切分的，好比 Lebesgue 积分的计算方法。现在我们可以按照 Riemann 积分的计算方法来表示一个时间序列的特征，于是就有学者把时间序列按照横轴切分成很多段，每一段使用某个简单函数（线性函数等）来表示，于是就有了以下的方法：

1. 分段线性逼近（Piecewise Linear Approximation）
2. 分段聚合逼近（Piecewise Aggregate Approximation）
3. 分段常数逼近（Piecewise Constant Approximation）

说到这几种算法，其实最本质的思想就是进行数据降维的工作，用少数的数据来进行原始时间序列的表示（Representation）。用数学化的语言来描述时间序列的数据降维（Data Reduction）就是：把原始的时间序列 $\{x_1,…,x_N\}$ 用 $\{x′_1,…,x′_D\}$来表示，其中$D<N$。那么后者就是原始序列的一种表示（representation）。

##### 分段聚合逼近（Piecewise Aggregate Approximation）—- 类似 Riemann 积分

在这种算法中，分段聚合逼近（Piecewise Aggregate Approximation）是一种非常经典的算法。假设原始的时间序列是 $C={x1,…,x_N}$，定义PAA的序列是：$\overline{C}=\{\overline{x}_1,...,\overline{x}_w\}$

其中，
$$
\overline{x}_i=\frac{w}{N}⋅\sum_{j=\frac{N}{w}(i−1)+1}^{\frac{N}{w}i}x_j$​
$$
在这里$1≤i≤w$用图像来表示那就是

![img](http://minio.vancode.top/vancode/algorithm/ad/clip1541314841.png)

至于分段线性逼近（Piecewise Linear Approximation）和分段常数逼近（Piecewise Constant Approximation），只需要在$\overline{x}_i$的定义上稍作修改即可。

##### 符号逼近（Symbolic Approximation）—- 类似 Riemann 积分

在推荐系统的特征工程里面，特征通常来说可以做归一化，二值化，离散化等操作。例如，用户的年龄特征，一般不会直接使用具体的年月日，而是划分为某个区间段，例如 0\~6（婴幼儿时期），7\~12（小学），13\~17（中学），18\~22（大学）等阶段。

其实在得到分段特征之后，分段特征在某种程度上来说依旧是某些连续值，能否把连续值划分为一些离散的值呢？于是就有学者使用一些符号来表示时间序列的关键特征，也就是所谓的符号表示法（Symbolic Representation）。下面来介绍经典的 SAX Representation。

如果我们希望使用 $\alpha$ 个符号，例如 ${l_1,…,l_\alpha}$来表示时间序列。同时考虑分布$N(0,1)$，用

$\{z_{1/\alpha},⋯,z_{(\alpha−1)/\alpha}\}$来表示 Gauss 曲线下方的一些点，而这些点把 Gauss 曲线下方的面积等分成了 $\alpha$段。

算法流程：

1. 正规化（normalization）：也就是该时间序列被映射到均值为零，方差为一的区间内。

2. 分段表示（PAA）：${x_1,⋯,x_N}⇒{\overline{x}_1,⋯,\overline{x}_w}$

3. 符号表示（SAX）：

   * 如果$\overline{x}_i<z_{1/α}$，那么$\hat{X}_i=l_1$；

   * 如果$z_{(j−1)/\alpha}≤\overline{x}_i<z_{j/α}$，那么$\hat{X}_i=l_j$；

   * 如果$\hat{x}_i≥z_{(\alpha−1)/\alpha}$，那么$\hat{X}_i=l_\alpha$。

于是，我们就可以用${l1,⋯,lα}$这$\alpha$个字母来表示原始的时间序列了。

![img](http://minio.vancode.top/vancode/algorithm/ad/clip1541315241.png)

#### 1.2.4 总特征

总的训练特征为：时间窗口特征 + OneHot特征 + Timestamp所属的星期作为特征，label选取时间窗口中间点的标签。因为选择了窗口数据的中间位置的点作为label值，所以在窗口移动过程中，在数据起始点和终结点会丢失 datasize-(windowsize/2) 个数据。（datasize为数据的总量，windowsize为时间窗口的大小)在最终提交的结果中，缺失的数据点的预测结果，用0补充。

### 1.3 分类方法

- 随机森林
- XGBoost
- DNN

### 1.4 聚类方法

#### 1.4.1 k-means聚类

k-均值聚类算法是一种发现数据簇的简单有效的方法。算法步骤如下：

1. 询问用户应当将数据集划分为多少个簇。

2. 随机分配k个记录成为初始簇中心位置。

3. 为每一个记录找到最近的簇中心，因此，从某种意义上来说，每个簇中心“拥有”一个记录的子集，从而表示一个数据集的划分。因此我们有k个簇$C_1,C_2,...,C_k$。

4. 对于k个簇中的每一个簇，找到簇质心，并将簇质心以新的簇中心位置更新。

5. 重复步骤3~5，直到收敛或终止。

> * 虽然其他标准也可以使用，但步骤3中的“最近的”标准通常是[欧氏距离](#1 欧氏距离)。
> * 步骤四中的簇中心的获取方法如下:假设我们有$n$个数据点$(a_1,b_1,c_1),(a_2,b_2,c_2),...,(a_n,b_n,c_n)$，这些点的质心就是这些点的重心，位于点$(\sum a_i/n,\sum b_i/n,\sum c_i/n)$,

当质心不再改变时，该算法终止。换句话说，对所有簇$C_1,C_2,...,C_k$,所有记录分别处于不同的簇且每个簇的中心不再发生变化时，程序终止。另外，当某些收敛准则得到满足时，该算法可能会终止，如**均方根误差(MSE)**无明显变化时。

##### 聚类指标

  $p\in C_i$表示簇i中的每个数据点，$m_j$代表簇i的质心(簇中心)，N是样本总数，k是簇的数量。

* **SSB** 簇之间的平方和
  $$
  SSB=\sum_{i=1}^k n_i\cdot d(m_i,M)^2
  $$

* **MSB** 簇之间的均方
  $$
  MSB=\frac{MSB}{k-1}
  $$

* **SSE** 误差平方和
  $$
  SSE=\sum_{i=1}^k\sum_{p\in C_i}d(p,m_i)^2
  $$

* **MSE** 均方误差
  $$
  MSE=\frac{SSE}{N-k}
  $$

* **伪-F统计量**
  $$
  F_{k-1,N-k}=\frac{MSB}{MSE}=\frac{SSB/k-1}{SSE/N-k}
  $$

##### 簇数确定方法

**1 Elbow方法**

​	手肘法的核心指标是SSE(sum of the squared errors，误差平方和)。随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。当然，这也是该方法被称为手肘法的原因。

**2 轮廓系数法**



## 2 长期环比（LS）

上面短期环比参考的是短期内的数据，而仅仅有短期内的数据是不够的，我们还需要参考更长时间内数据的总体走势。

通常使用一条曲线对该趋势进行拟合来反应曲线的走势，如果新的数据打破了这种趋势，使曲线变得不平滑，则该点就出现了异常。曲线拟合的方法有很多，比如回归、moving average……。在这里，我们使用EWMA，即指数权重移动平均方法来拟合曲线。在EWMA中，下一点的平均值是由上一点的平均值，加上当前点的实际值修正而来。对于每一个EWMA值，每个数据的权重是不一样的，最近的数据将拥有越高的权重。

有了平均值之后，我们就可以使用3-sigma理论来判断新的input是否超过了容忍范围。比较实际的值是否超出了这个范围就可以知道是否可以告警了。

[![img](http://minio.vancode.top/vancode/algorithm/ad/clip1541172227.png)](http://image.rexking6.top/img/http://minio.vancode.top/vancode/algorithm/ad/clip1541172227.png)

**自己理解：长期环比除了通过拟合、EWMA等方法，ARIMA和RNN也应该能适用**

## 3 同比(chain)

很多监控项都具有一定的周期性，其中以一天为周期的情况比较常见，比如lvs流量在早上4点最低，而在晚上11点最高。为了将监控项的周期性考虑进去，我们选取了某个监控项过去14天的数据。对于某个时刻，将得到14个点可以作为参考值，我们记为$x_i$，其中$i=1,…,14$。

我们先考虑静态阈值的方法来判断input是否异常（突增和突减）。如果input比过去14天同一时刻的最小值乘以一个阈值还小，就会认为该输入为异常点（突减）；而如果input比过去14天同一时刻的最大值乘以一个阈值还大，就会认为该输入为异常点（突增）。

[![img](http://minio.vancode.top/vancode/algorithm/ad/clip1541172603.png)](http://image.rexking6.top/img/http://minio.vancode.top/vancode/algorithm/ad/clip1541172603.png)

静态阈值的方法是根据历史经验得出的值，实际中如何给max threshold和min threshold 是一个需要讨论的话题。根据目前动态阈值的经验规则来说，取平均值是一个比较好的思路。

**自己的理解：同比的方法只能针对周期性的曲线使用，但是同比的方法个人认为只能用来辅助预测，更多的应该是用来检测异常。**

## 4 同比振幅(CA)

同比的方法遇到有些情况就不能检测出异常。比如今天是11.18日，过去14天的历史曲线必然会比今天的曲线低很多。那么今天出了一个小故障，曲线下跌了，相对于过去14天的曲线仍然是高很多的。这样的故障使用方法二就检测不出来，那么我们将如何改进我们的方法呢？一个直觉的说法是，两个曲线虽然不一样高，但是“长得差不多”。那么怎么利用这种“长得差不多”呢？那就是振幅了。

怎么计算$t$时刻的振幅呢？ 我们使用$\frac{x(t)–x(t−1)}{x(t−1)}$来表示振幅。举个例子，例如$t$时刻的流量为900bit，$t−1$时刻的是1000bit，那么可以计算出掉线人数是10%。如果参考过去14天的数据，我们会得到14个振幅值。使用14个振幅的绝对值作为标准，如果$m$时刻的振幅$\frac{m(t)–m(t−1)}{m(t−1)}>amplitude∗threshold$并且$m$时刻的振幅大于$0$，则我们认为该时刻发生突增，而如果$m$时刻的振幅大于$amplitude∗threshold$并且$m$时刻的振幅小于0，则认为该时刻发生突减。其中，
$$
amplitude=max[|\frac{x_i(t)−x_i(t−1)}{x_i(t−1)}|],x=1,2,...,14
$$


## 5 算法组合

上面介绍了四种方法，这四种方法里面，SS和LS是针对非周期性数据的验证方法，而chain和CA是针对周期性数据的验证方法。那这四种方法应该如何选择和使用呢？下面我们介绍两种使用方法：

一、根据周期性的不同来选择合适的方法。这种方法需要首先验证序列是否具有周期性，如果具有周期性则进入左边分支的检测方法，如果没有周期性，则选择进入右分支的检测方法。

[![img](http://minio.vancode.top/vancode/algorithm/ad/clip1541239974.png)](http://image.rexking6.top/img/http://minio.vancode.top/vancode/algorithm/ad/clip1541239974.png)

上面涉及到了如何检测数据周期的问题，可以使用差分的方法来检测数据是否具有周期性。比如取最近两天的数据来做差分，如果是周期数据，差分后就可以消除波动，然后结合方差阈值判断的判断方法来确定数据的周期性。当然，如果数据波动范围比较大，可以在差分之前先对数据进行归一化（比如z-score）。

二、不区分周期性，直接根据“少数服从多数”的方法来去检测，这种方法比较好理解，在此就不说明了。

[![img](http://minio.vancode.top/vancode/algorithm/ad/clip1541239998.png)](http://image.rexking6.top/img/http://minio.vancode.top/vancode/algorithm/ad/clip1541239998.png)

## 6 其他方法

### Smoothed z-score algorithm

[![img](http://minio.vancode.top/vancode/algorithm/ad/clip1541169228.png)](http://image.rexking6.top/img/http://minio.vancode.top/vancode/algorithm/ad/clip1541169228.png)

主要思想：

1. 利用过去一段历史窗口针对下个节点值做预测（利用平均值，方差信息），若是其超过了一定的阈值，则是个异常点。
2. 对异常点的数值进行平滑，以便评估下下个点是否为异常点。因为不做平滑，由于当前是个异常点，对平均值、方差影响较大，若是下一个点仍是异常点，可能不会识别。

https://zhuanlan.zhihu.com/p/39453139

## 7 相空间重构

- [时间序列模型之相空间重构模型](https://zhuanlan.zhihu.com/p/32910931)





